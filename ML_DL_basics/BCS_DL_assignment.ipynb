{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjKg47GBdXY9"
      },
      "source": [
        "# **Brain and Cognitive Society, IIT Kanpur**\n",
        "## **Introduction to Deep Learning Workshop**\n",
        "**This python notebook is an assingment on ML/DL**\n",
        "\n",
        "In this assingment you will solve a **regression** problem of predicting House prices using basic python libraries, and build a **neural network** for handwritten digit identification using **TensorFlow**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Linear Regression**\n",
        "We will use Linear regression for predicting house prices\n",
        "\n",
        "We are using a Kaggle dataset- https://www.kaggle.com/harlfoxem/housesalesprediction"
      ],
      "metadata": {
        "id": "FSRalact4xj4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets import required Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n"
      ],
      "metadata": {
        "id": "J8MRCcJY2btt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Dataset Preparation**"
      ],
      "metadata": {
        "id": "1MzAhMmE6UHE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Execute this cell for loading dataset in a pandas dataframe\n",
        "\n",
        "from IPython.display import clear_output\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=16x6-8Znn2T50zFwVvKlzsdN7Jd1hpjct' -O Linear_regression_dataset\n",
        "\n",
        "data_df = pd.read_csv(\"Linear_regression_dataset\")"
      ],
      "metadata": {
        "id": "LiWI-2py554R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets have a quick Look at dataset\n",
        "\n",
        "print(\"(No of rows, No of Columns) = \",data_df.shape)\n",
        "data_df.head()"
      ],
      "metadata": {
        "id": "TAodxbYX7AKf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So there are **19** features (of course we will not use id as feature :) ), and 1 variable to predict(price)\n",
        "\n",
        "But note that the **date** column contain strings so first we will remove T00.. part from it and than convert it to numpy array."
      ],
      "metadata": {
        "id": "gsJaooGZ7pUV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_df['date'] = ...                                         # Remove T000000 part from data column. Hint: search about .str.replace() method. :)  \n",
        "\n",
        "data_array = ...                                              # Create a numpy array which does not have \"id\" field\n",
        "assert (data_array.shape == (21613,20))\n",
        "\n",
        "data_df.head()"
      ],
      "metadata": {
        "id": "FNFNf3jT7oxW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now the next task is **normalization**.\n",
        "\n",
        "We will scale each column of dataset by x -> (x-u)/s\n",
        "\n",
        "where u is mean(x), and s is standard deviation of u"
      ],
      "metadata": {
        "id": "xsBZxZ4x-oBR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mean = ...                                  # this should be an array, each entry should be mean of a column\n",
        "sd = ...                                    # this should be an array, each entry should be standard deviation of a column\n",
        "\n",
        "data_array_norm = (data_array - mean)/sd\n",
        "\n",
        "print(data_array_norm.shape)"
      ],
      "metadata": {
        "id": "u7GZV-0T_zCy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The last step is to make train and test dataset and to create seperate vector for price"
      ],
      "metadata": {
        "id": "VCQTrNIgAlPv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "labels = ...                                                                                                            # extract the price column from data\n",
        "\n",
        "x_array_norm = ...                                                                                                      # delete the price column from data_array_norm. Hint: use np.delete()\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x_array_norm,labels,test_size=0.15,random_state=42,shuffle=True)    # splitting data into test and train set.\n",
        "\n",
        "print(x_train.shape,x_test.shape,y_train.shape,y_test.shape)"
      ],
      "metadata": {
        "id": "dJyX5QOFBRg5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Loss and gradient descent**\n",
        "We will use mean squared error(MSE) as loss\n",
        "\n",
        "Use the gradient descent algorithm which you learned from tutorials\n",
        "\n",
        "Your task is to complete the following functions"
      ],
      "metadata": {
        "id": "iAdW-22ZDcdU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def loss(y_pred,y_true):\n",
        "  \"\"\"\n",
        "  input:\n",
        "  y_pred = [array] predicted value of y\n",
        "  y_true = [array] ground truth\n",
        "  \n",
        "  output:\n",
        "  mse: [scalar] the MES loss\n",
        "  \"\"\"\n",
        "  mse = ...                      # fill code here\n",
        "\n",
        "  return mse"
      ],
      "metadata": {
        "id": "ufoIQOpeEFQx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def y(x,a,b):\n",
        "  \"\"\"\n",
        "  This function should return predicted value of y = ax+b\n",
        "  input:\n",
        "  x: [array] the feature vector of shape (m,n)\n",
        "  a: [array] weights of shape (n,)\n",
        "  b: [scalar] bias\n",
        "  \n",
        "  output:\n",
        "  y_pred: [array] predicted value of y of shape (m,)\n",
        "  \"\"\"\n",
        "\n",
        "  m,n = x.shape\n",
        "  y_pred = ...                   # fill code here\n",
        "\n",
        "  assert(y_pred.shape == (m,))\n",
        "  return y_pred"
      ],
      "metadata": {
        "id": "a6LogNz5E28X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient(x,a,b,y_true):\n",
        "  \"\"\"\n",
        "  This function shoud return gradient of loss\n",
        "  input:\n",
        "  x: [array] the feature vector of shape (m,n)\n",
        "  a: [array] weights of shape (n,)\n",
        "  b: [scalar] bias\n",
        "  y_true: [array] ground truth of shape (m,)\n",
        "\n",
        "  output:\n",
        "  grad: [tuple] a tuple (derivative with respect to a[array of shape(n,)], derivative with respect to b[scalar])\n",
        "  \"\"\"\n",
        "  m,n = x.shape\n",
        "  yp = y(x,a,b)\n",
        "\n",
        "  da = ...              # write code to calculate derivative of loss with respect to a\n",
        "  db = ...              # write code to calculate derivative of loss with respect to b\n",
        "\n",
        "  assert(da.shape ==(n,))\n",
        "  return (da,db)"
      ],
      "metadata": {
        "id": "lYnPROu8Gxwi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_descent(x,y_true,learning_rate=0.01,epochs = 10):\n",
        "  \"\"\"\n",
        "  This function perfroms gradient descent and minimizes loss\n",
        "  input:\n",
        "  x: [array] the feature vector of shape (m,n)\n",
        "  y_true: [array] ground truth of shape (m,)\n",
        "  \n",
        "  output:\n",
        "  loss: [array] of size (epochs,)\n",
        "  weights: [tuple] (a,b)\n",
        "  \"\"\"\n",
        "  m,n = x.shape\n",
        "  loss_mse = []                                 # initialize empty list to store loss\n",
        "  a = ...                                       # initialize a- weights and b- bias\n",
        "  b = ...\n",
        "\n",
        "  for i in range(epochs):\n",
        "    # calculate derivative using gradient() function\n",
        "    # apply gradient descent now to update a and b\n",
        "    \n",
        "    l_mse = ....                                # calculate loss at this point\n",
        "    loss_mse.append(l_mse)\n",
        "\n",
        "    print(\"Epoch \",i+1,\" Completed!\",\"loss = \",l_mse)\n",
        "  \n",
        "  print(\"Training completed!!\")\n",
        "\n",
        "  assert(a.shape==(n,))\n",
        "\n",
        "  return (loss_mse,a,b)"
      ],
      "metadata": {
        "id": "km_z3ojKKQdj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Training** "
      ],
      "metadata": {
        "id": "VsR5XLl_WVu4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = ...              # tweak this!!!\n",
        "learn_rate = ...          # choose learning rate wisely otherwise loss may diverge!!\n",
        "\n",
        "train_loss,a,b = gradient_descent(...)"
      ],
      "metadata": {
        "id": "5A9mqkqLWU27"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Evaluation and Visualization**\n",
        "Lets plot how loss varies with epochs\n"
      ],
      "metadata": {
        "id": "TDH-7NHQT50f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss = loss(...)\n",
        "\n",
        "print(\"Loss on test data = \",test_loss)\n",
        "\n",
        "# Visualization of loss\n",
        "\n",
        "plt.plot(...)                   # plot loss versus epochs\n",
        "plt.title(\"Training Loss\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "d7JRB_nJUEkV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Deep Learning**\n",
        "In this section We will build a simple multilayer perceptron network(**MLP**) in TensorFlow"
      ],
      "metadata": {
        "id": "iQ5wPjPb57pb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EGoZEKIbdXZD"
      },
      "outputs": [],
      "source": [
        "# Lets import the required libraries\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ThuZ51gEdXZF"
      },
      "source": [
        "### **Load Dataset**\n",
        "We will be using MNIST dataset of handwritten digits\n",
        "\n",
        "Just run the cell below to load dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xj3J8Dp-dXZG"
      },
      "outputs": [],
      "source": [
        "mnist = keras.datasets.mnist\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "print(\"No. of training examples = \",x_train.shape[0])\n",
        "print(\"Size of each image in dataset = \",x_train.shape[1:])\n",
        "print(\"No. of test examples = \",x_test.shape[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XX9xW4ardXZG"
      },
      "outputs": [],
      "source": [
        "# Run this cell to visualize some of the images from dataset\n",
        "\n",
        "n = 5    # = no. of images to visualize\n",
        "\n",
        "index = np.random.choice(x_train.shape[0],5)  # choose random index\n",
        "print(\"label: \",end=\"\")\n",
        "\n",
        "for i,ind in enumerate(index):\n",
        "    plt.subplot(1,n,i+1)\n",
        "    plt.imshow(x_train[ind],cmap=\"gray\")\n",
        "    plt.axis(\"off\")\n",
        "    print(y_train[ind],end=\"       \")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQcA9i9YdXZH"
      },
      "source": [
        "#### Preprocess dataset\n",
        "Since we are building a MLP model the input to the model should be a vector rather than a 28 by 28 matrix.\n",
        "\n",
        "So your **First Task** is to flatten the images\n",
        "\n",
        "(Hint: use *reshape()* method of arrays...)\n",
        "\n",
        "Next, create validation dataset out of training dataset.\n",
        "\n",
        "You can use 50K images for training and 10K for validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RXnkfE6gdXZI"
      },
      "outputs": [],
      "source": [
        "# Flatten the images into 1-d vectors\n",
        "\n",
        "x_train_flatten = ...                                       # flatten the images of training set \n",
        "x_test_flatten = ...                                        # flatten th eimages of test set\n",
        "\n",
        "\n",
        "# Divide the training data into training and validation data....\n",
        "\n",
        "n_validation = 10000                                        # choose number of images to be used for validation\n",
        "\n",
        "x_validation = ...\n",
        "y_validation = ...\n",
        "\n",
        "x_train_flatten = ...\n",
        "y_train = ...\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMGl2aq3dXZJ"
      },
      "source": [
        "### **Build a model**\n",
        "You can choose whatever architechure you want, but ensure that it is **not too deep** as that will take too much time to train and **not too shallow** as that will give very low accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A7yr3nwTdXZK"
      },
      "outputs": [],
      "source": [
        "model = keras.models.Sequential([\n",
        "    ...\n",
        "])\n",
        "\n",
        "# Make a graphical representation of the model...\n",
        "keras.utils.plot_model(model,show_shapes=True)\n",
        "model.summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oDNvKB6dXZL"
      },
      "source": [
        "#### Compile and Train\n",
        "Choose an optimizer- method that minimizes loss function\n",
        "\n",
        "**adam** optimizer is one of the popular choices. You should read about these online"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7DM9i_F5dXZL"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer=\"...\",loss = \"...\",metrics=[\"accuracy\"])\n",
        "\n",
        "n_epochs = ...              # set number of epochs\n",
        "batch_size = 512            # you can tweak with these parametrs\n",
        "history = model.fit(...)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7QTWTtoVdXZM"
      },
      "source": [
        "### **Evaluate**\n",
        "Evaluate your model on test data.\n",
        "\n",
        "And Show some results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bhuBGWg-dXZM"
      },
      "outputs": [],
      "source": [
        "results = model.evaluate(...)\n",
        "print(\"Loss = \",results[0])\n",
        "print(\"Accuracy = \",results[1]*100,\"%\")\n",
        "\n",
        "# Plot Accuracy...\n",
        "plt.plot(..., label=\"Training accuracy\")\n",
        "plt.plot(..., label=\"validation Accuracy\")\n",
        "plt.title(\"Model accuracy\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Similarly write code to plot loss...\n",
        "...\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Hjr0CBhdXZN"
      },
      "source": [
        "Lets show our results on images from testing dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sEs1cVAHdXZN"
      },
      "outputs": [],
      "source": [
        "n = ...   # = no. of images to see predictions on\n",
        "\n",
        "index = np.random.choice(...)  # choose random index from test data\n",
        "print(\"label: \")\n",
        "\n",
        "for i,ind in enumerate(index):\n",
        "    plt.subplot(1,n,i+1)\n",
        "    plt.imshow(...)             # fill code to show images from test set\n",
        "    plt.axis(\"off\")\n",
        "    print(y_test[ind],end=\"       \")\n",
        "\n",
        "plt.show()\n",
        "print(\"Predicted value: \")\n",
        "\n",
        "# Now lets print the predictions\n",
        "\n",
        "for i,ind in enumerate(index):\n",
        "    # write code to predict and print digit in image\n",
        "    # Hint: the output of the model is a 10-d vector which gives probabilties\n",
        "    # The digit in the image would be the class for which probability is hghest...\n",
        "\n",
        "    digit = ...\n",
        "    print(digit,end=\"      \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ne8-aBvHdXZO"
      },
      "source": [
        "That's it you have completed the assignment !!\n",
        "\n",
        "We hope that you learned something from this exercise\n",
        "\n",
        "### Credits:\n",
        "\n",
        "**Leaders:**\n",
        "\n",
        "Mohit Kulkarni\n",
        "\n",
        "Shivanshu Tyagi\n",
        "\n",
        "**Scretaries:**\n",
        "\n",
        "Sahil Bansal\n",
        "\n",
        "Shashwat Gupta\n",
        "\n",
        "Rashmi Sharma"
      ]
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
    },
    "kernelspec": {
      "display_name": "Python 3.8.10 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "orig_nbformat": 4,
    "colab": {
      "name": "BCS_DL_assignment",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}